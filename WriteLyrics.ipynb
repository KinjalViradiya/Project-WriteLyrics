{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Lyrics\n",
    "\n",
    "we are going to write songs based on previous data.\n",
    "\n",
    "[Link for Dataset](https://www.kaggle.com/mousehead/songlyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "filename=\"lyrics.txt\"\n",
    "text= open(filename, 'r', encoding='utf-8').read()\n",
    "text=text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 68\n"
     ]
    }
   ],
   "source": [
    "# Create a sorted list of the characters\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 260342\n"
     ]
    }
   ],
   "source": [
    "# Output the length of the corpus\n",
    "print('length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 100000\n"
     ]
    }
   ],
   "source": [
    "# training for first 8000000 text \n",
    "text = text[:100000]\n",
    "print('length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of all unique chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters:  100000\n",
      "Total vocab:  68\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(text)\n",
    "n_vocabs = len(chars)\n",
    "print(\"Total characters: \", n_chars)\n",
    "print(\"Total vocab: \", n_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns:  99900\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length , 1):\n",
    "    seq_in = text[i:i + seq_length]\n",
    "    seq_out =text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append([char_to_int[seq_out]])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since LSTMs accept values in the form (no_of_sampels, time_steps, no_of_features), therefore\n",
    "reshape dataX to this form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape dataX\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X/float(n_vocabs)\n",
    "\n",
    "# one hot encoding using np_utils\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99900, 100, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99900, 67)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoints and callbacks\n",
    "filepath=\"SavedModel/weights-imporvement-{epoch: 02d}-{loss: .4f}-from-class.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"SavedModel/weights-imporvement- 50- 1.3979-from-class.hdf5\" # add the name of your best trained saved model's name here\n",
    "model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/150\n",
      "99900/99900 [==============================] - 1145s 11ms/step - loss: 1.3875\n",
      "\n",
      "Epoch 00051: loss improved from 1.39788 to 1.38745, saving model to SavedModel/weights-imporvement- 51- 1.3875-from-class.hdf5\n",
      "Epoch 52/150\n",
      "99900/99900 [==============================] - 1117s 11ms/step - loss: 1.3830\n",
      "\n",
      "Epoch 00052: loss improved from 1.38745 to 1.38296, saving model to SavedModel/weights-imporvement- 52- 1.3830-from-class.hdf5\n",
      "Epoch 53/150\n",
      "99900/99900 [==============================] - 1124s 11ms/step - loss: 1.3725\n",
      "\n",
      "Epoch 00053: loss improved from 1.38296 to 1.37247, saving model to SavedModel/weights-imporvement- 53- 1.3725-from-class.hdf5\n",
      "Epoch 54/150\n",
      "99900/99900 [==============================] - 2891s 29ms/step - loss: 1.3643\n",
      "\n",
      "Epoch 00054: loss improved from 1.37247 to 1.36431, saving model to SavedModel/weights-imporvement- 54- 1.3643-from-class.hdf5\n",
      "Epoch 55/150\n",
      "99900/99900 [==============================] - 897s 9ms/step - loss: 1.3635\n",
      "\n",
      "Epoch 00055: loss improved from 1.36431 to 1.36351, saving model to SavedModel/weights-imporvement- 55- 1.3635-from-class.hdf5\n",
      "Epoch 56/150\n",
      "99900/99900 [==============================] - 899s 9ms/step - loss: 1.3545\n",
      "\n",
      "Epoch 00056: loss improved from 1.36351 to 1.35446, saving model to SavedModel/weights-imporvement- 56- 1.3545-from-class.hdf5\n",
      "Epoch 57/150\n",
      "99900/99900 [==============================] - 907s 9ms/step - loss: 1.3490\n",
      "\n",
      "Epoch 00057: loss improved from 1.35446 to 1.34902, saving model to SavedModel/weights-imporvement- 57- 1.3490-from-class.hdf5\n",
      "Epoch 58/150\n",
      "99900/99900 [==============================] - 901s 9ms/step - loss: 1.3373\n",
      "\n",
      "Epoch 00058: loss improved from 1.34902 to 1.33734, saving model to SavedModel/weights-imporvement- 58- 1.3373-from-class.hdf5\n",
      "Epoch 59/150\n",
      "99900/99900 [==============================] - 921s 9ms/step - loss: 1.3327\n",
      "\n",
      "Epoch 00059: loss improved from 1.33734 to 1.33267, saving model to SavedModel/weights-imporvement- 59- 1.3327-from-class.hdf5\n",
      "Epoch 60/150\n",
      "99900/99900 [==============================] - 902s 9ms/step - loss: 1.3319\n",
      "\n",
      "Epoch 00060: loss improved from 1.33267 to 1.33187, saving model to SavedModel/weights-imporvement- 60- 1.3319-from-class.hdf5\n",
      "Epoch 61/150\n",
      "99900/99900 [==============================] - 898s 9ms/step - loss: 1.3226\n",
      "\n",
      "Epoch 00061: loss improved from 1.33187 to 1.32259, saving model to SavedModel/weights-imporvement- 61- 1.3226-from-class.hdf5\n",
      "Epoch 62/150\n",
      "99900/99900 [==============================] - 897s 9ms/step - loss: 1.3224\n",
      "\n",
      "Epoch 00062: loss improved from 1.32259 to 1.32241, saving model to SavedModel/weights-imporvement- 62- 1.3224-from-class.hdf5\n",
      "Epoch 63/150\n",
      "99900/99900 [==============================] - 896s 9ms/step - loss: 1.3127\n",
      "\n",
      "Epoch 00063: loss improved from 1.32241 to 1.31275, saving model to SavedModel/weights-imporvement- 63- 1.3127-from-class.hdf5\n",
      "Epoch 64/150\n",
      "99900/99900 [==============================] - 900s 9ms/step - loss: 1.3076\n",
      "\n",
      "Epoch 00064: loss improved from 1.31275 to 1.30760, saving model to SavedModel/weights-imporvement- 64- 1.3076-from-class.hdf5\n",
      "Epoch 65/150\n",
      "99900/99900 [==============================] - 895s 9ms/step - loss: 1.3011\n",
      "\n",
      "Epoch 00065: loss improved from 1.30760 to 1.30111, saving model to SavedModel/weights-imporvement- 65- 1.3011-from-class.hdf5\n",
      "Epoch 66/150\n",
      "99900/99900 [==============================] - 906s 9ms/step - loss: 1.3008\n",
      "\n",
      "Epoch 00066: loss improved from 1.30111 to 1.30080, saving model to SavedModel/weights-imporvement- 66- 1.3008-from-class.hdf5\n",
      "Epoch 67/150\n",
      "99900/99900 [==============================] - 893s 9ms/step - loss: 1.2997\n",
      "\n",
      "Epoch 00067: loss improved from 1.30080 to 1.29967, saving model to SavedModel/weights-imporvement- 67- 1.2997-from-class.hdf5\n",
      "Epoch 68/150\n",
      "99900/99900 [==============================] - 894s 9ms/step - loss: 1.2954\n",
      "\n",
      "Epoch 00068: loss improved from 1.29967 to 1.29540, saving model to SavedModel/weights-imporvement- 68- 1.2954-from-class.hdf5\n",
      "Epoch 69/150\n",
      "99900/99900 [==============================] - 893s 9ms/step - loss: 1.2847\n",
      "\n",
      "Epoch 00069: loss improved from 1.29540 to 1.28474, saving model to SavedModel/weights-imporvement- 69- 1.2847-from-class.hdf5\n",
      "Epoch 70/150\n",
      "99900/99900 [==============================] - 895s 9ms/step - loss: 1.2790\n",
      "\n",
      "Epoch 00070: loss improved from 1.28474 to 1.27898, saving model to SavedModel/weights-imporvement- 70- 1.2790-from-class.hdf5\n",
      "Epoch 71/150\n",
      "99900/99900 [==============================] - 895s 9ms/step - loss: 1.2777\n",
      "\n",
      "Epoch 00071: loss improved from 1.27898 to 1.27766, saving model to SavedModel/weights-imporvement- 71- 1.2777-from-class.hdf5\n",
      "Epoch 72/150\n",
      "99900/99900 [==============================] - 894s 9ms/step - loss: 1.2743\n",
      "\n",
      "Epoch 00072: loss improved from 1.27766 to 1.27426, saving model to SavedModel/weights-imporvement- 72- 1.2743-from-class.hdf5\n",
      "Epoch 73/150\n",
      "99900/99900 [==============================] - 894s 9ms/step - loss: 1.2793\n",
      "\n",
      "Epoch 00073: loss did not improve from 1.27426\n",
      "Epoch 74/150\n",
      "99900/99900 [==============================] - 898s 9ms/step - loss: 1.2724\n",
      "\n",
      "Epoch 00074: loss improved from 1.27426 to 1.27236, saving model to SavedModel/weights-imporvement- 74- 1.2724-from-class.hdf5\n",
      "Epoch 75/150\n",
      "99900/99900 [==============================] - 895s 9ms/step - loss: 1.2654\n",
      "\n",
      "Epoch 00075: loss improved from 1.27236 to 1.26542, saving model to SavedModel/weights-imporvement- 75- 1.2654-from-class.hdf5\n",
      "Epoch 76/150\n",
      "99900/99900 [==============================] - 897s 9ms/step - loss: 1.2532\n",
      "\n",
      "Epoch 00076: loss improved from 1.26542 to 1.25316, saving model to SavedModel/weights-imporvement- 76- 1.2532-from-class.hdf5\n",
      "Epoch 77/150\n",
      "99900/99900 [==============================] - 894s 9ms/step - loss: 1.2561\n",
      "\n",
      "Epoch 00077: loss did not improve from 1.25316\n",
      "Epoch 78/150\n",
      "99900/99900 [==============================] - 896s 9ms/step - loss: 1.2538\n",
      "\n",
      "Epoch 00078: loss did not improve from 1.25316\n",
      "Epoch 79/150\n",
      "99900/99900 [==============================] - 894s 9ms/step - loss: 1.2480\n",
      "\n",
      "Epoch 00079: loss improved from 1.25316 to 1.24796, saving model to SavedModel/weights-imporvement- 79- 1.2480-from-class.hdf5\n",
      "Epoch 80/150\n",
      "99900/99900 [==============================] - 893s 9ms/step - loss: 1.2480\n",
      "\n",
      "Epoch 00080: loss did not improve from 1.24796\n",
      "Epoch 81/150\n",
      "99900/99900 [==============================] - 893s 9ms/step - loss: 1.2465\n",
      "\n",
      "Epoch 00081: loss improved from 1.24796 to 1.24647, saving model to SavedModel/weights-imporvement- 81- 1.2465-from-class.hdf5\n",
      "Epoch 82/150\n",
      "99900/99900 [==============================] - 895s 9ms/step - loss: 1.2371\n",
      "\n",
      "Epoch 00082: loss improved from 1.24647 to 1.23712, saving model to SavedModel/weights-imporvement- 82- 1.2371-from-class.hdf5\n",
      "Epoch 83/150\n",
      "99900/99900 [==============================] - 895s 9ms/step - loss: 1.2442\n",
      "\n",
      "Epoch 00083: loss did not improve from 1.23712\n",
      "Epoch 84/150\n",
      "99900/99900 [==============================] - 892s 9ms/step - loss: 1.2350\n",
      "\n",
      "Epoch 00084: loss improved from 1.23712 to 1.23503, saving model to SavedModel/weights-imporvement- 84- 1.2350-from-class.hdf5\n",
      "Epoch 85/150\n",
      "99900/99900 [==============================] - 891s 9ms/step - loss: 1.2354\n",
      "\n",
      "Epoch 00085: loss did not improve from 1.23503\n",
      "Epoch 86/150\n",
      "99900/99900 [==============================] - 893s 9ms/step - loss: 1.2271\n",
      "\n",
      "Epoch 00086: loss improved from 1.23503 to 1.22705, saving model to SavedModel/weights-imporvement- 86- 1.2271-from-class.hdf5\n",
      "Epoch 87/150\n",
      "99900/99900 [==============================] - 892s 9ms/step - loss: 1.2246\n",
      "\n",
      "Epoch 00087: loss improved from 1.22705 to 1.22455, saving model to SavedModel/weights-imporvement- 87- 1.2246-from-class.hdf5\n",
      "Epoch 88/150\n",
      "99900/99900 [==============================] - 899s 9ms/step - loss: 1.2256\n",
      "\n",
      "Epoch 00088: loss did not improve from 1.22455\n",
      "Epoch 89/150\n",
      "99900/99900 [==============================] - 893s 9ms/step - loss: 1.2230\n",
      "\n",
      "Epoch 00089: loss improved from 1.22455 to 1.22304, saving model to SavedModel/weights-imporvement- 89- 1.2230-from-class.hdf5\n",
      "Epoch 90/150\n",
      "99900/99900 [==============================] - 895s 9ms/step - loss: 1.2220\n",
      "\n",
      "Epoch 00090: loss improved from 1.22304 to 1.22199, saving model to SavedModel/weights-imporvement- 90- 1.2220-from-class.hdf5\n",
      "Epoch 91/150\n",
      "99900/99900 [==============================] - 894s 9ms/step - loss: 1.2183\n",
      "\n",
      "Epoch 00091: loss improved from 1.22199 to 1.21829, saving model to SavedModel/weights-imporvement- 91- 1.2183-from-class.hdf5\n",
      "Epoch 92/150\n",
      "99900/99900 [==============================] - 893s 9ms/step - loss: 1.2234\n",
      "\n",
      "Epoch 00092: loss did not improve from 1.21829\n",
      "Epoch 93/150\n",
      "99900/99900 [==============================] - 891s 9ms/step - loss: 1.2193\n",
      "\n",
      "Epoch 00093: loss did not improve from 1.21829\n",
      "Epoch 94/150\n",
      "99900/99900 [==============================] - 893s 9ms/step - loss: 1.2144\n",
      "\n",
      "Epoch 00094: loss improved from 1.21829 to 1.21437, saving model to SavedModel/weights-imporvement- 94- 1.2144-from-class.hdf5\n",
      "Epoch 95/150\n",
      "99900/99900 [==============================] - 895s 9ms/step - loss: 1.2098\n",
      "\n",
      "Epoch 00095: loss improved from 1.21437 to 1.20981, saving model to SavedModel/weights-imporvement- 95- 1.2098-from-class.hdf5\n",
      "Epoch 96/150\n",
      "99900/99900 [==============================] - 892s 9ms/step - loss: 1.2116\n",
      "\n",
      "Epoch 00096: loss did not improve from 1.20981\n",
      "Epoch 97/150\n",
      "99900/99900 [==============================] - 891s 9ms/step - loss: 1.2210\n",
      "\n",
      "Epoch 00097: loss did not improve from 1.20981\n",
      "Epoch 98/150\n",
      "99900/99900 [==============================] - 891s 9ms/step - loss: 1.2037\n",
      "\n",
      "Epoch 00098: loss improved from 1.20981 to 1.20369, saving model to SavedModel/weights-imporvement- 98- 1.2037-from-class.hdf5\n",
      "Epoch 99/150\n",
      "99900/99900 [==============================] - 891s 9ms/step - loss: 1.4272\n",
      "\n",
      "Epoch 00099: loss did not improve from 1.20369\n",
      "Epoch 100/150\n",
      "99900/99900 [==============================] - 895s 9ms/step - loss: 2.7199\n",
      "\n",
      "Epoch 00100: loss did not improve from 1.20369\n",
      "Epoch 101/150\n",
      "99900/99900 [==============================] - 894s 9ms/step - loss: 2.4877\n",
      "\n",
      "Epoch 00101: loss did not improve from 1.20369\n",
      "Epoch 102/150\n",
      "99900/99900 [==============================] - 892s 9ms/step - loss: 1.5157\n",
      "\n",
      "Epoch 00102: loss did not improve from 1.20369\n",
      "Epoch 103/150\n",
      "99900/99900 [==============================] - 899s 9ms/step - loss: 1.3195\n",
      "\n",
      "Epoch 00103: loss did not improve from 1.20369\n",
      "Epoch 104/150\n",
      "99900/99900 [==============================] - 890s 9ms/step - loss: 1.2319\n",
      "\n",
      "Epoch 00104: loss did not improve from 1.20369\n",
      "Epoch 105/150\n",
      "99900/99900 [==============================] - 892s 9ms/step - loss: 1.2363\n",
      "\n",
      "Epoch 00105: loss did not improve from 1.20369\n",
      "Epoch 106/150\n",
      "99900/99900 [==============================] - 892s 9ms/step - loss: 1.2437\n",
      "\n",
      "Epoch 00106: loss did not improve from 1.20369\n",
      "Epoch 107/150\n",
      "99900/99900 [==============================] - 894s 9ms/step - loss: 1.2329\n",
      "\n",
      "Epoch 00107: loss did not improve from 1.20369\n",
      "Epoch 108/150\n",
      "99900/99900 [==============================] - 891s 9ms/step - loss: 1.2272\n",
      "\n",
      "Epoch 00108: loss did not improve from 1.20369\n",
      "Epoch 109/150\n",
      "99900/99900 [==============================] - 888s 9ms/step - loss: 1.2211\n",
      "\n",
      "Epoch 00109: loss did not improve from 1.20369\n",
      "Epoch 110/150\n",
      "99900/99900 [==============================] - 889s 9ms/step - loss: 1.2156\n",
      "\n",
      "Epoch 00110: loss did not improve from 1.20369\n",
      "Epoch 111/150\n",
      "99900/99900 [==============================] - 890s 9ms/step - loss: 1.2268\n",
      "\n",
      "Epoch 00111: loss did not improve from 1.20369\n",
      "Epoch 112/150\n",
      "99900/99900 [==============================] - 894s 9ms/step - loss: 1.2332\n",
      "\n",
      "Epoch 00112: loss did not improve from 1.20369\n",
      "Epoch 113/150\n",
      "99900/99900 [==============================] - 892s 9ms/step - loss: 1.8978\n",
      "\n",
      "Epoch 00113: loss did not improve from 1.20369\n",
      "Epoch 114/150\n",
      "99900/99900 [==============================] - 890s 9ms/step - loss: 1.6676\n",
      "\n",
      "Epoch 00114: loss did not improve from 1.20369\n",
      "Epoch 115/150\n",
      "99900/99900 [==============================] - 898s 9ms/step - loss: 1.3070\n",
      "\n",
      "Epoch 00115: loss did not improve from 1.20369\n",
      "Epoch 116/150\n",
      "99900/99900 [==============================] - 890s 9ms/step - loss: 1.2150\n",
      "\n",
      "Epoch 00116: loss did not improve from 1.20369\n",
      "Epoch 117/150\n",
      "99900/99900 [==============================] - 890s 9ms/step - loss: 1.2293\n",
      "\n",
      "Epoch 00117: loss did not improve from 1.20369\n",
      "Epoch 118/150\n",
      "99900/99900 [==============================] - 892s 9ms/step - loss: 1.2216\n",
      "\n",
      "Epoch 00118: loss did not improve from 1.20369\n",
      "Epoch 119/150\n",
      "99900/99900 [==============================] - 904s 9ms/step - loss: 1.2316\n",
      "\n",
      "Epoch 00119: loss did not improve from 1.20369\n",
      "Epoch 120/150\n",
      "99900/99900 [==============================] - 900s 9ms/step - loss: 1.2202\n",
      "\n",
      "Epoch 00120: loss did not improve from 1.20369\n",
      "Epoch 121/150\n",
      "99900/99900 [==============================] - 889s 9ms/step - loss: 1.2245\n",
      "\n",
      "Epoch 00121: loss did not improve from 1.20369\n",
      "Epoch 122/150\n",
      "99900/99900 [==============================] - 887s 9ms/step - loss: 1.2248\n",
      "\n",
      "Epoch 00122: loss did not improve from 1.20369\n",
      "Epoch 123/150\n",
      "99900/99900 [==============================] - 888s 9ms/step - loss: 1.2145\n",
      "\n",
      "Epoch 00123: loss did not improve from 1.20369\n",
      "Epoch 124/150\n",
      "99900/99900 [==============================] - 892s 9ms/step - loss: 1.2170\n",
      "\n",
      "Epoch 00124: loss did not improve from 1.20369\n",
      "Epoch 125/150\n",
      "99900/99900 [==============================] - 889s 9ms/step - loss: 1.1999\n",
      "\n",
      "Epoch 00125: loss improved from 1.20369 to 1.19991, saving model to SavedModel/weights-imporvement- 125- 1.1999-from-class.hdf5\n",
      "Epoch 126/150\n",
      "99900/99900 [==============================] - 889s 9ms/step - loss: 1.2099\n",
      "\n",
      "Epoch 00126: loss did not improve from 1.19991\n",
      "Epoch 127/150\n",
      "99900/99900 [==============================] - 892s 9ms/step - loss: 1.2131\n",
      "\n",
      "Epoch 00127: loss did not improve from 1.19991\n",
      "Epoch 128/150\n",
      "99900/99900 [==============================] - 951s 10ms/step - loss: 1.2031\n",
      "\n",
      "Epoch 00128: loss did not improve from 1.19991\n",
      "Epoch 129/150\n",
      "99900/99900 [==============================] - 930s 9ms/step - loss: 1.4104\n",
      "\n",
      "Epoch 00129: loss did not improve from 1.19991\n",
      "Epoch 130/150\n",
      "99900/99900 [==============================] - 927s 9ms/step - loss: 3.7169\n",
      "\n",
      "Epoch 00130: loss did not improve from 1.19991\n",
      "Epoch 131/150\n",
      "99900/99900 [==============================] - 913s 9ms/step - loss: 3.1474\n",
      "\n",
      "Epoch 00131: loss did not improve from 1.19991\n",
      "Epoch 132/150\n",
      "99900/99900 [==============================] - 911s 9ms/step - loss: 3.0510\n",
      "\n",
      "Epoch 00132: loss did not improve from 1.19991\n",
      "Epoch 133/150\n",
      "99900/99900 [==============================] - 920s 9ms/step - loss: 2.8697\n",
      "\n",
      "Epoch 00133: loss did not improve from 1.19991\n",
      "Epoch 134/150\n",
      "99900/99900 [==============================] - 927s 9ms/step - loss: 2.7448\n",
      "\n",
      "Epoch 00134: loss did not improve from 1.19991\n",
      "Epoch 135/150\n",
      "99900/99900 [==============================] - 919s 9ms/step - loss: 2.6555\n",
      "\n",
      "Epoch 00135: loss did not improve from 1.19991\n",
      "Epoch 136/150\n",
      "22208/99900 [=====>........................] - ETA: 12:11 - loss: 2.5899"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-6bfa85cfbea9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training the LSTM\n",
    "\n",
    "\n",
    "# model.fit(X, y, epochs=150, batch_size=64, callbacks=callbacks_list,initial_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Text from pretrained/post training your LSTM\n",
    "\n",
    "filename = \"SavedModel/weights-imporvement- 125- 1.1999-from-class.hdf5\" # add the name of your best trained saved model's name here\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SEED:\n",
      "\" on a minute, ah!\n",
      "do anybody feel bad for bill cosby?\n",
      "did he forget the names just like steve harvey? \"\n",
      "\n",
      " (yeezy, yeezy, yeezy, i meed to stop again\n",
      "bacy gor my friek, rurped\n",
      "to do the god of the tame thing i know it's no jnde when i get it\n",
      "we goin' on my bickc\n",
      "in the damn mames\n",
      "we roow this doun shgt'a whlle pict the ducking that i hot a baw berter clame me what i don't tell the tame thing\n",
      "i know it's no puac\n",
      "lasgie' well so mige\n",
      "sooe like you can't say this for a mew friend and ii you got a man with a bitch\n",
      "is all off in a lomer but i'm on the ptesenc\n",
      "i'm foing on the nast we was up well i got the way that she doccr on the fay from the carkroiny\n",
      "oow she got me on the tame thing\n",
      "i know it's no jnre, the was fron the wale i wake up and i can't get to and sood a thggt, rhete niggas that lise tha\n",
      "THE END.\n"
     ]
    }
   ],
   "source": [
    "# set up a random seed for starting\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "\n",
    "print(\"INPUT SEED:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[val] for val in pattern]), \"\\\"\")\n",
    "print()\n",
    "# generate characters from the generated output of LSTM\n",
    "for i in range(700):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x/float(n_vocabs)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1: len(pattern)]\n",
    "print(\"\\nTHE END.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
