{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Lyrics\n",
    "\n",
    "we are going to write songs based on previous data.\n",
    "\n",
    "[Link for Dataset](https://www.kaggle.com/mousehead/songlyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading a songs file excel\n",
    "df = pd.read_csv('Dataset/songdata.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining text columns as remaining are not important\n",
    "text = df['text'].str.cat(sep='\\n').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 50\n"
     ]
    }
   ],
   "source": [
    "# Create a sorted list of the characters\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 68056106\n"
     ]
    }
   ],
   "source": [
    "# Output the length of the corpus\n",
    "print('length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 1000000\n"
     ]
    }
   ],
   "source": [
    "# training for first 8000000 text \n",
    "text = text[:1000000]\n",
    "print('length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of all unique chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters:  1000000\n",
      "Total vocab:  50\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(text)\n",
    "n_vocabs = len(chars)\n",
    "print(\"Total characters: \", n_chars)\n",
    "print(\"Total vocab: \", n_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns:  999960\n"
     ]
    }
   ],
   "source": [
    "seq_length = 40\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length , 1):\n",
    "    seq_in = text[i:i + seq_length]\n",
    "    seq_out =text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append([char_to_int[seq_out]])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since LSTMs accept values in the form (no_of_sampels, time_steps, no_of_features), therefore\n",
    "reshape dataX to this form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape dataX\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X/float(n_vocabs)\n",
    "\n",
    "# one hot encoding using np_utils\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999960, 40, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999960, 50)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoints and callbacks\n",
    "filepath=\"SavedModel/weights-imporvement-{epoch: 02d}-{loss: .4f}-from-class.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/30\n",
      "999960/999960 [==============================] - 2933s 3ms/step - loss: 2.3228\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.32279, saving model to SavedModel/weights-imporvement- 1- 2.3228-from-class.hdf5\n",
      "Epoch 2/30\n",
      "999960/999960 [==============================] - 2916s 3ms/step - loss: 1.9330\n",
      "\n",
      "Epoch 00002: loss improved from 2.32279 to 1.93303, saving model to SavedModel/weights-imporvement- 2- 1.9330-from-class.hdf5\n",
      "Epoch 3/30\n",
      "999960/999960 [==============================] - 2892s 3ms/step - loss: 1.7894\n",
      "\n",
      "Epoch 00003: loss improved from 1.93303 to 1.78942, saving model to SavedModel/weights-imporvement- 3- 1.7894-from-class.hdf5\n",
      "Epoch 4/30\n",
      "999960/999960 [==============================] - 2938s 3ms/step - loss: 1.7056\n",
      "\n",
      "Epoch 00004: loss improved from 1.78942 to 1.70562, saving model to SavedModel/weights-imporvement- 4- 1.7056-from-class.hdf5\n",
      "Epoch 5/30\n",
      "999960/999960 [==============================] - 2900s 3ms/step - loss: 1.6453\n",
      "\n",
      "Epoch 00005: loss improved from 1.70562 to 1.64529, saving model to SavedModel/weights-imporvement- 5- 1.6453-from-class.hdf5\n",
      "Epoch 6/30\n",
      "999960/999960 [==============================] - 2872s 3ms/step - loss: 1.6023\n",
      "\n",
      "Epoch 00006: loss improved from 1.64529 to 1.60231, saving model to SavedModel/weights-imporvement- 6- 1.6023-from-class.hdf5\n",
      "Epoch 7/30\n",
      "999960/999960 [==============================] - 2876s 3ms/step - loss: 1.5655\n",
      "\n",
      "Epoch 00007: loss improved from 1.60231 to 1.56554, saving model to SavedModel/weights-imporvement- 7- 1.5655-from-class.hdf5\n",
      "Epoch 8/30\n",
      "999960/999960 [==============================] - 2876s 3ms/step - loss: 1.5380\n",
      "\n",
      "Epoch 00008: loss improved from 1.56554 to 1.53802, saving model to SavedModel/weights-imporvement- 8- 1.5380-from-class.hdf5\n",
      "Epoch 9/30\n",
      "999960/999960 [==============================] - 2869s 3ms/step - loss: 1.5140\n",
      "\n",
      "Epoch 00009: loss improved from 1.53802 to 1.51405, saving model to SavedModel/weights-imporvement- 9- 1.5140-from-class.hdf5\n",
      "Epoch 10/30\n",
      "999960/999960 [==============================] - 2862s 3ms/step - loss: 1.4944\n",
      "\n",
      "Epoch 00010: loss improved from 1.51405 to 1.49438, saving model to SavedModel/weights-imporvement- 10- 1.4944-from-class.hdf5\n",
      "Epoch 11/30\n",
      "999960/999960 [==============================] - 2873s 3ms/step - loss: 1.4776\n",
      "\n",
      "Epoch 00011: loss improved from 1.49438 to 1.47765, saving model to SavedModel/weights-imporvement- 11- 1.4776-from-class.hdf5\n",
      "Epoch 12/30\n",
      "999960/999960 [==============================] - 2867s 3ms/step - loss: 1.4644\n",
      "\n",
      "Epoch 00012: loss improved from 1.47765 to 1.46441, saving model to SavedModel/weights-imporvement- 12- 1.4644-from-class.hdf5\n",
      "Epoch 13/30\n",
      "999960/999960 [==============================] - 2856s 3ms/step - loss: 1.4507\n",
      "\n",
      "Epoch 00013: loss improved from 1.46441 to 1.45069, saving model to SavedModel/weights-imporvement- 13- 1.4507-from-class.hdf5\n",
      "Epoch 14/30\n",
      "999960/999960 [==============================] - 2860s 3ms/step - loss: 1.4415\n",
      "\n",
      "Epoch 00014: loss improved from 1.45069 to 1.44150, saving model to SavedModel/weights-imporvement- 14- 1.4415-from-class.hdf5\n",
      "Epoch 15/30\n",
      "999960/999960 [==============================] - 2868s 3ms/step - loss: 1.4308\n",
      "\n",
      "Epoch 00015: loss improved from 1.44150 to 1.43077, saving model to SavedModel/weights-imporvement- 15- 1.4308-from-class.hdf5\n",
      "Epoch 16/30\n",
      "999960/999960 [==============================] - 2872s 3ms/step - loss: 1.4228\n",
      "\n",
      "Epoch 00016: loss improved from 1.43077 to 1.42281, saving model to SavedModel/weights-imporvement- 16- 1.4228-from-class.hdf5\n",
      "Epoch 17/30\n",
      "999960/999960 [==============================] - 2867s 3ms/step - loss: 1.4150\n",
      "\n",
      "Epoch 00017: loss improved from 1.42281 to 1.41500, saving model to SavedModel/weights-imporvement- 17- 1.4150-from-class.hdf5\n",
      "Epoch 18/30\n",
      "999960/999960 [==============================] - 2870s 3ms/step - loss: 1.4086\n",
      "\n",
      "Epoch 00018: loss improved from 1.41500 to 1.40858, saving model to SavedModel/weights-imporvement- 18- 1.4086-from-class.hdf5\n",
      "Epoch 19/30\n",
      "999960/999960 [==============================] - 2863s 3ms/step - loss: 1.4022\n",
      "\n",
      "Epoch 00019: loss improved from 1.40858 to 1.40219, saving model to SavedModel/weights-imporvement- 19- 1.4022-from-class.hdf5\n",
      "Epoch 20/30\n",
      "999960/999960 [==============================] - 2852s 3ms/step - loss: 1.3962\n",
      "\n",
      "Epoch 00020: loss improved from 1.40219 to 1.39617, saving model to SavedModel/weights-imporvement- 20- 1.3962-from-class.hdf5\n",
      "Epoch 21/30\n",
      "999960/999960 [==============================] - 2867s 3ms/step - loss: 1.3908\n",
      "\n",
      "Epoch 00021: loss improved from 1.39617 to 1.39076, saving model to SavedModel/weights-imporvement- 21- 1.3908-from-class.hdf5\n",
      "Epoch 22/30\n",
      "999960/999960 [==============================] - 2869s 3ms/step - loss: 1.3851\n",
      "\n",
      "Epoch 00022: loss improved from 1.39076 to 1.38508, saving model to SavedModel/weights-imporvement- 22- 1.3851-from-class.hdf5\n",
      "Epoch 23/30\n",
      "999960/999960 [==============================] - 2864s 3ms/step - loss: 1.3808\n",
      "\n",
      "Epoch 00023: loss improved from 1.38508 to 1.38081, saving model to SavedModel/weights-imporvement- 23- 1.3808-from-class.hdf5\n",
      "Epoch 24/30\n",
      "999960/999960 [==============================] - 2869s 3ms/step - loss: 1.3758\n",
      "\n",
      "Epoch 00024: loss improved from 1.38081 to 1.37582, saving model to SavedModel/weights-imporvement- 24- 1.3758-from-class.hdf5\n",
      "Epoch 25/30\n",
      "999960/999960 [==============================] - 2862s 3ms/step - loss: 1.3719\n",
      "\n",
      "Epoch 00025: loss improved from 1.37582 to 1.37187, saving model to SavedModel/weights-imporvement- 25- 1.3719-from-class.hdf5\n",
      "Epoch 26/30\n",
      "999960/999960 [==============================] - 2875s 3ms/step - loss: 1.3688\n",
      "\n",
      "Epoch 00026: loss improved from 1.37187 to 1.36879, saving model to SavedModel/weights-imporvement- 26- 1.3688-from-class.hdf5\n",
      "Epoch 27/30\n",
      "999960/999960 [==============================] - 2865s 3ms/step - loss: 1.3657\n",
      "\n",
      "Epoch 00027: loss improved from 1.36879 to 1.36566, saving model to SavedModel/weights-imporvement- 27- 1.3657-from-class.hdf5\n",
      "Epoch 28/30\n",
      "999960/999960 [==============================] - 2861s 3ms/step - loss: 1.3609\n",
      "\n",
      "Epoch 00028: loss improved from 1.36566 to 1.36088, saving model to SavedModel/weights-imporvement- 28- 1.3609-from-class.hdf5\n",
      "Epoch 29/30\n",
      "999960/999960 [==============================] - 2866s 3ms/step - loss: 1.3581\n",
      "\n",
      "Epoch 00029: loss improved from 1.36088 to 1.35809, saving model to SavedModel/weights-imporvement- 29- 1.3581-from-class.hdf5\n",
      "Epoch 30/30\n",
      "999960/999960 [==============================] - 2878s 3ms/step - loss: 1.3533\n",
      "\n",
      "Epoch 00030: loss improved from 1.35809 to 1.35335, saving model to SavedModel/weights-imporvement- 30- 1.3533-from-class.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1fa3c214ac8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the LSTM\n",
    "\n",
    "model.fit(X, y, epochs=30, batch_size=128, callbacks=callbacks_list) # do try out at diff epoch and batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SEED:\n",
      "\" i know i've got to try  \n",
      "and make it wit \"\n",
      "\n",
      "h the world  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the soad  \n",
      "the sound of the s\n",
      "THE END.\n"
     ]
    }
   ],
   "source": [
    "# set up a random seed for starting\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "\n",
    "print(\"INPUT SEED:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[val] for val in pattern]), \"\\\"\")\n",
    "print()\n",
    "# generate characters from the generated output of LSTM\n",
    "for i in range(2000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x/float(n_vocabs)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1: len(pattern)]\n",
    "print(\"\\nTHE END.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
